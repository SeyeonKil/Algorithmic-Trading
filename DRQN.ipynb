{"cells":[{"cell_type":"markdown","id":"a8191b1a","metadata":{"id":"a8191b1a"},"source":["# Added:\n","\n","- LSTM layer and linear layer\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"WiAzyom19zRY","metadata":{"id":"WiAzyom19zRY"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"8c8aca73","metadata":{"id":"8c8aca73"},"source":["# Install packages"]},{"cell_type":"code","execution_count":null,"id":"e91d7670","metadata":{"id":"e91d7670"},"outputs":[],"source":["pip install gym\n","pip install yfinance\n","pip install numpy --upgrade #check if numpy is the most recent version"]},{"cell_type":"code","execution_count":null,"id":"EeE1znX7ULDR","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeE1znX7ULDR","outputId":"eadeee06-3757-4e76-f08c-165b8e4d73c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"SrFG3WNwUPrE","metadata":{"id":"SrFG3WNwUPrE"},"outputs":[],"source":["pathSP = \"/content/drive/MyDrive/Colab Notebooks/Thesis code/data/SP2014-2016.csv\""]},{"cell_type":"code","execution_count":null,"id":"NMVbH6qKUP2P","metadata":{"id":"NMVbH6qKUP2P"},"outputs":[],"source":["pathMSFT = \"/content/drive/MyDrive/Colab Notebooks/Thesis code/data/MSFT2014-2016.csv\""]},{"cell_type":"markdown","id":"b732fd00","metadata":{"id":"b732fd00"},"source":["# Trading_env"]},{"cell_type":"code","execution_count":null,"id":"9f471b97","metadata":{"id":"9f471b97"},"outputs":[],"source":["import random\n","from statistics import mean\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","class TradingSystem_v0:\n","    def __init__(self, returns_data, k_value, action_dim, initial_balance,mode,stock):\n","        self.mode = mode  # test or train\n","        self.initial_balance = initial_balance\n","        self.balance = self.initial_balance\n","        self.k = k_value\n","        self.tickers = stock\n","        self.curr_position = 0\n","        self.action_space_dim = action_dim\n","        self.total_steps = returns_data.shape[0] - self.k\n","        self.current_step = 0\n","        self.reward = 0.0\n","        self.core = float(returns_data.iloc[returns_data.shape[0]-1,3])\n","        self.dev = float(np.std(returns_data.iloc[:,3]))\n","        self.trans_cost = 0.0005 # transaction cost\n","        self.slippage_rate = 0.001 # slippage rate\n","        for i in range(0,returns_data.shape[1]):\n","            col = np.asarray(returns_data.iloc[:,i])\n","            norm_value = preprocessing.normalize([col]).squeeze()\n","            for j in range(0,returns_data.shape[0]):\n","                returns_data.iloc[j,i] = norm_value[j]\n","        returns_data = returns_data.assign(MA = MA(returns_data.iloc[:,3],10), MACD = MACD(returns_data.iloc[:,3],10,20)\n","        ,ROC = ROC(returns_data.iloc[:,3],10), SR = SR(returns_data.iloc[:,3],10))\n","        self.initial_state = returns_data.iloc[0,:]\n","        self.state = self.initial_state\n","        self.batch = 16\n","        self.r_ts = returns_data\n","        self.is_terminal = False\n","\n","    # write step function that returns obs(next state), reward, is_done\n","    def step(self, action):\n","        self.current_step += 1\n","        if self.current_step == self.total_steps:\n","            self.is_terminal = True\n","\n","        self.pos_change = action - self.action_space_dim//2  # get action(position) of the actor\n","\n","        self.curr_position = self.curr_position + self.pos_change\n","\n","        if self.pos_change == 0:\n","\n","            close_price = self.r_ts.iloc[self.current_step,3]\n","            close_price_bf = self.r_ts.iloc[self.current_step-1,3]\n","            self.reward =  self.pos_change * (close_price - close_price_bf)\n","        else:\n","            close_price = self.r_ts.iloc[self.current_step,3]\n","            close_price_bf = self.r_ts.iloc[self.current_step-1,3]\n","            self.reward =  self.pos_change * (close_price - close_price_bf)  - abs(self.pos_change)* (self.trans_cost+self.slippage_rate)\n","\n","        self.state = self.r_ts.iloc[self.current_step+1,:]\n","\n","        return self.state, self.reward, self.is_terminal\n","\n","    def reset(self):\n","        self.r_ts = self.r_ts\n","        self.total_steps = len(self.r_ts) - self.k\n","        self.balance = self.initial_balance\n","        self.current_step = 0\n","        self.initial_state = self.r_ts.iloc[0,:]\n","        self.state = self.initial_state\n","        self.reward = 0.0\n","        self.is_terminal = False\n","        return self.state\n","\n"]},{"cell_type":"markdown","id":"6175267b","metadata":{"id":"6175267b"},"source":["# dqn"]},{"cell_type":"code","execution_count":null,"id":"d796d28d","metadata":{"id":"d796d28d"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import random\n","import math\n","import numpy as np\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_dim=128):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(state_dim, 64)  # input layer\n","        self.fc2 = nn.Linear(64, hidden_dim)  # hidden layer 1\n","        self.fc3 = nn.Linear(hidden_dim, 128)  # hidden layer 2\n","        self.lstm_cells = nn.LSTMCell(128, 128) # lstm layer (returns hidden state and cell state)\n","        self.fc4 = nn.Linear(128, action_dim) # connects lstm layer to linear layer\n","\n","    def forward(self, x):\n","        # activation function\n","        x = torch.nn.functional.relu(self.fc1(x))\n","        x = torch.nn.functional.relu(self.fc2(x))\n","        x = torch.nn.functional.relu(self.fc3(x))\n","        hx,cx = self.lstm_cells(x)\n","        # x = hx.clone().detach()\n","        # x = torch.tensor(hx) (gives warning)\n","\n","\n","        return self.fc4(hx)\n","\n","\n","class ReplayBuffer:\n","    def __init__(self, capacity):\n","        self.capacity = capacity  # capacity of buffer\n","        self.buffer = []  # replay buffer\n","        self.position = 0\n","\n","    def push(self, state, action, reward, next_state, done):\n","        ''' replay buffer is a queue (LIFO)\n","        '''\n","        if len(self.buffer) < self.capacity:\n","            self.buffer.append(None)\n","        self.buffer[self.position] = (state, action, reward, next_state, done)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        batch = random.sample(self.buffer, batch_size)\n","        state, action, reward, next_state, done = zip(*batch)\n","        return state, action, reward, next_state, done\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","\n","class DQN:\n","    def __init__(self, state_dim, action_dim, cfg):\n","\n","        self.action_dim = action_dim\n","        self.device = cfg.device  # cpu or gpu\n","        self.gamma = cfg.gamma  # discount factor\n","        self.frame_idx = 0  # attenuation\n","        self.epsilon = lambda frame_idx: cfg.epsilon_end + \\\n","                                         (cfg.epsilon_start - cfg.epsilon_end) * \\\n","                                         math.exp(-1. * frame_idx / cfg.epsilon_decay)\n","        self.batch_size = cfg.batch_size\n","        self.policy_net = MLP(state_dim, action_dim, hidden_dim=cfg.hidden_dim).to(self.device)\n","        self.target_net = MLP(state_dim, action_dim, hidden_dim=cfg.hidden_dim).to(self.device)\n","        for target_param, param in zip(self.target_net.parameters(),\n","                                       self.policy_net.parameters()):  # copy parameters to target net\n","            target_param.data.copy_(param.data)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=cfg.lr) # optimizer\n","        self.memory = ReplayBuffer(cfg.memory_capacity)  # experience replay\n","\n","    def choose_action(self, state):\n","        self.frame_idx += 1\n","        if random.random() > self.epsilon(self.frame_idx):\n","            with torch.no_grad():\n","                state = torch.tensor([state], device=self.device, dtype=torch.float32)   # received observation\n","                q_values = self.policy_net(state)\n","                action = q_values.max(1)[1].item()  # choose the action with maximum q-value\n","        else:\n","            action = random.randrange(self.action_dim)\n","        return action\n","\n","    def update(self):\n","        if len(self.memory) < self.batch_size:\n","            return\n","        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(\n","            self.batch_size)\n","        # transfer to tensor\n","        state_batch = torch.tensor(state_batch, device=self.device, dtype=torch.float)\n","        action_batch = torch.tensor(action_batch, device=self.device).unsqueeze(1)\n","        reward_batch = torch.tensor(reward_batch, device=self.device, dtype=torch.float)\n","        next_state_batch = torch.tensor(next_state_batch, device=self.device, dtype=torch.float)\n","        done_batch = torch.tensor(np.float32(done_batch), device=self.device)\n","        q_values = self.policy_net(state_batch).gather(dim=1, index=action_batch)\n","        next_q_values = self.target_net(next_state_batch).max(1)[0].detach()\n","        # calculate the expected q-value, for final state, done_batch[0]=1 and the corresponding\n","        # expected_q_value equals to reward\n","        expected_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)\n","        loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))\n","        # update the network\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        for param in self.policy_net.parameters():  # avoid gradient explosion by using clip\n","            param.grad.data.clamp_(-1, 1)\n","        self.optimizer.step()\n","\n","    def save(self, path):\n","        torch.save(self.target_net.state_dict(), path + 'dqn_checkpoint.pth')\n","\n","    def load(self, path):\n","        self.target_net.load_state_dict(torch.load(path + 'dqn_checkpoint.pth'))\n","        for target_param, param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n","            param.data.copy_(target_param.data)\n","\n"]},{"cell_type":"markdown","id":"5be83e66","metadata":{"id":"5be83e66"},"source":["# Main"]},{"cell_type":"code","execution_count":null,"id":"6aa75d54","metadata":{"id":"6aa75d54"},"outputs":[],"source":["import sys\n","import os\n","import os.path\n","\n","\n","curr_path = os.path.abspath('')\n","\n","import gym\n","import torch\n","import numpy as np\n","import random\n","import yfinance as yf\n","import datetime as dt\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","curr_time = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","random.seed(11)\n","\n","\n","\n","class Config:\n","    '''\n","    hyperparameters\n","    '''\n","\n","    def __init__(self):\n","        ################################## env hyperparameters ###################################\n","        self.algo_name = 'DQN' # algorithmic name\n","        self.env_name = 'TradingSystem_v0' # environment name\n","        self.device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() else \"cpu\")  # examine GPU\n","        self.seed = 11 # random seed\n","        self.train_eps = 100 # number of training episode\n","        self.state_space_dim = 8 # state space size (K-value)\n","        self.initial_balance = 1000000\n","        self.action_space_dim = 3 # action space size (only odd number can be applied)\n","        ################################################################################\n","\n","        ################################## algo hyperparameters ###################################\n","        self.gamma = 0.95  # discount factor\n","        self.epsilon_start = 0.90  # start epsilon of e-greedy policy\n","        self.epsilon_end = 0.01  # end epsilon of e-greedy policy\n","        self.epsilon_decay = 500  # attenuation rate of epsilon in e-greedy policy\n","        self.lr = 0.0001  # learning rate\n","        self.memory_capacity = 1000  # capacity of experience replay\n","        self.batch_size = 64  # size of mini-batch SGD\n","        self.target_update = 10  # update frequency of target network\n","        self.hidden_dim = 128  # dimension of hidden layer\n","        ################################################################################\n","\n","        ################################# save path ##############################\n","        self.result_path = curr_path + \"/outputs/\" + self.env_name + \\\n","                           '/' + curr_time + '/results/'\n","        self.model_path = curr_path + \"/outputs/\" + self.env_name + \\\n","                          '/' + curr_time + '/models/'\n","        self.save = True  # whether to save the image\n","        ################################################################################\n","\n","\n","def env_agent_config(data, cfg, mode,stock):\n","    ''' create environment and agent\n","    '''\n","    env = TradingSystem_v0(data, cfg.state_space_dim, cfg.action_space_dim, cfg.initial_balance, mode, stock)\n","    agent = DQN(cfg.state_space_dim, cfg.action_space_dim, cfg)\n","\n","    if cfg.seed != 0:  # set random seeds\n","        torch.manual_seed(cfg.seed)\n","        np.random.seed(cfg.seed)\n","    return env, agent\n","\n","\n","def train(cfg, env, agent):\n","    ''' training\n","    '''\n","    print('Start Training!')\n","    print(f'Environment：{cfg.env_name}, Algorithm：{cfg.algo_name}, Device：{cfg.device}, Stock: {env.tickers}')\n","    rewards = []  # record total rewards\n","    ma_rewards = []  # record moving average total rewards\n","    stocks = env.tickers\n","    ep_reward = 0\n","    state = env.reset()\n","    for i_ep in range(cfg.train_eps):\n","        ep_reward = 0\n","        state = env.reset()\n","        while True:\n","            action = agent.choose_action(state)\n","            next_state, reward, done = env.step(action)\n","            agent.memory.push(state, action, reward, next_state, done)  # save transition\n","            state = next_state\n","            agent.update()\n","            ep_reward +=  reward\n","            if done:\n","                break\n","        if (i_ep + 1) % cfg.target_update == 0:  # update target network\n","            agent.target_net.load_state_dict(agent.policy_net.state_dict())\n","        sum_rewards = env.core * env.batch * (1+ep_reward) + env.dev * ep_reward\n","        rewards.append(sum_rewards)\n","        if ma_rewards:\n","            ma_rewards.append(0.9 * ma_rewards[-1] + 0.1 * sum_rewards)\n","        else:\n","            ma_rewards.append(sum_rewards)\n","        if (i_ep + 1) % 10 == 0:\n","            print('Episode：{}/{}, Reward：{}'.format(i_ep + 1, cfg.train_eps, sum_rewards))\n","    print('Finish Training!')\n","    return rewards, ma_rewards\n","\n","\n","def test(cfg, env, agent):\n","    print('Start Testing!')\n","    print(f'Environment：{cfg.env_name}, Algorithm：{cfg.algo_name}, Device：{cfg.device},Stock: {env.tickers}')\n","    ############# Test does not use e-greedy policy, so we set epsilon to 0 ###############\n","    cfg.epsilon_start = 0.0\n","    cfg.epsilon_end = 0.0\n","    ################################################################################\n","    stocks = env.tickers\n","    returns = []\n","    ep_reward = 0\n","    state = env.reset()\n","    while True:\n","        action = agent.choose_action(state)\n","        next_state, reward, done = env.step(action)\n","        state = next_state\n","        ep_reward += reward\n","        if done:\n","            break\n","\n","    sum_rewards = ep_reward*env.dev + env.core\n","    returns.append(sum_rewards)\n","    print(f\"Stock：{stocks}，Return：{sum_rewards:.1f}\")\n","    print('Finish Testing!')\n","    return stocks, sum_rewards\n"]},{"cell_type":"markdown","id":"30bbb6a7","metadata":{"id":"30bbb6a7"},"source":["# Running files"]},{"cell_type":"markdown","id":"iGjQok1Nez-7","metadata":{"id":"iGjQok1Nez-7"},"source":["# Indicators"]},{"cell_type":"code","execution_count":null,"id":"SmkEeZAccyMJ","metadata":{"id":"SmkEeZAccyMJ"},"outputs":[],"source":["def MA(sorted_data, period):\n","\tma_list = []\n","\tfor i in range(len(sorted_data)):\n","\t\tif(i < period - 1):\n","\t\t\tma_list.append(sorted_data[i])\n","\t\t\t#ma_list.append(None)\n","\t\telse:\n","\t\t\tma_sum = 0.0\n","\t\t\tfor j in range(i, i - period, -1):\n","\t\t\t\tma_sum += sorted_data[j]\n","\t\t\tcurr_ma = ma_sum / period\n","\t\t\tma_list.append(curr_ma)\n","\treturn ma_list\n","\n","def EMA(data, period):\n","    sorted_data = np.array([data[i] for i in sorted(data.keys())])\n","    sma = np.mean(sorted_data[:period])\n","    ema_list = []\n","    smooth_weighting = 2.0 / (period + 1)\n","    for i in range(len(sorted_data)):\n","        if i < period - 1:\n","            ema_list.append(sma)\n","        else:\n","            new_ema = (sorted_data[i] - ema_list[-1]) * smooth_weighting + ema_list[-1]\n","            ema_list.append(new_ema)\n","    assert len(ema_list) == len(sorted_data)\n","    return np.array(ema_list)\n","\n","def MACD(sorted_data, period_1, period_2):\n","\tperiod_1_ema_list = EMA(sorted_data, period_1)\n","\tperiod_2_ema_list = EMA(sorted_data, period_2)\n","\tassert len(period_1_ema_list) == len(period_2_ema_list)\n","\tmacd_list = period_1_ema_list - period_2_ema_list\n","\treturn macd_list\n","\n","\n","def ROC(data, period):\n","    roc_list = []\n","    for i in range(len(data)):\n","        if i == 0:\n","            roc_list.append(0.0)\n","        elif i < period:\n","            curr_roc = (data[i] - data[0]) / data[0]\n","            roc_list.append(curr_roc)\n","        else:\n","            curr_roc = (data[i] - data[i - period]) / data[i - period]\n","            roc_list.append(curr_roc)\n","    return roc_list\n","\n","def SR(sorted_data, period, risk_free_rate=0.007):\n","    # trading hours per day\n","    risk_free_rate = risk_free_rate / (6.5 * 60)\n","    sr_list = []\n","    for i in range(len(sorted_data)):\n","        if i < period:\n","            sr_list.append(0.0)\n","        else:\n","            expected_return_list = []\n","            for j in range(i, i - period, -1):\n","                perc_change = (sorted_data[j] - sorted_data[j - 1]) / sorted_data[j - 1]\n","                expected_return_list.append(perc_change)\n","            expected_return_list = np.array(expected_return_list)\n","            std = np.std(expected_return_list)\n","            if std != 0.0:\n","                sr_list.append((np.mean(expected_return_list) - risk_free_rate) / std)\n","            else:\n","                sr_list.append(0.0)\n","    return sr_list\n"]},{"cell_type":"markdown","id":"gEg3TDF4QjvM","metadata":{"id":"gEg3TDF4QjvM"},"source":["# Uploading Dataset"]},{"cell_type":"code","execution_count":null,"id":"lnzykvjw3Agk","metadata":{"id":"lnzykvjw3Agk"},"outputs":[],"source":["import pandas as pd\n","from sklearn import preprocessing\n","import math\n","\n","### S&P 500 ###\n","SP = pd.read_csv(r'/Users/gilseyeon/Downloads/data/SP2014-2016.csv')\n","\n","#SP = pd.read_csv(pathSP)\n","oo = range(0,len(SP),30)\n","returns_SP = SP.iloc[oo,].reset_index(drop=True)\n","\n","\n","returns_SP = returns_SP.iloc[:,2:6] # remove time stamps\n","\n","\n","#splitting train and test set\n","splitter = math.ceil(returns_SP.shape[0]*0.9)\n","SP_train = returns_SP.iloc[0:splitter,:]\n","SP_test = returns_SP.iloc[splitter+1:,:].reset_index(drop=True)\n","\n","\n","\n","### Microsoft ###\n","MSFT = pd.read_csv(r'/Users/gilseyeon/Downloads/data/MSFT2014-2016.csv')\n","#MSFT = pd.read_csv(pathMSFT)\n","dd = range(0,len(MSFT),6)\n","returns_MSFT = MSFT.iloc[dd,].reset_index(drop=True)\n","\n","\n","returns_MSFT = returns_MSFT.iloc[:,1:5] # remove time stamps\n","\n","\n","#splitting train and test set\n","splitter = math.ceil(returns_MSFT.shape[0]*0.9)\n","MSFT_train = returns_MSFT.iloc[0:splitter,:]\n","MSFT_test = returns_MSFT.iloc[splitter+1:,:].reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"id":"lx38tjPr-lvO","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":479},"id":"lx38tjPr-lvO","outputId":"f8711759-df4e-49f3-df0c-280a1ba1acaa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start Training!\n","Environment：TradingSystem_v0, Algorithm：DQN, Device：cpu, Stock: SP500\n","Episode：10/100, Reward：21726.97176509192\n","Episode：20/100, Reward：25033.448567125466\n","Episode：30/100, Reward：23098.21736353873\n","Episode：40/100, Reward：26452.34049062942\n","Episode：50/100, Reward：25129.777057493924\n","Episode：60/100, Reward：25438.955410266884\n","Episode：70/100, Reward：24978.65396195801\n","Episode：80/100, Reward：25235.001330867293\n","Episode：90/100, Reward：24016.024910429416\n","Episode：100/100, Reward：25236.06555030652\n","Finish Training!\n"]}],"source":["# Training SP500\n","cfg = Config()\n","env, agent = env_agent_config(SP_train, cfg, 'train',\"SP500\")\n","rewards_train_SP, ma_rewards_train_SP = train(cfg, env, agent)\n"]},{"cell_type":"code","execution_count":null,"id":"Pxg_mE8fQ5EQ","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":170},"id":"Pxg_mE8fQ5EQ","outputId":"2ee32ed5-83b2-47b4-f0d3-6d3e96699469"},"outputs":[{"data":{"text/plain":["[1310.8527648273762,\n"," 17051.445476931276,\n"," 20353.5016751403,\n"," 20564.11394795461,\n"," 21971.272169949025,\n"," 21481.5525654397,\n"," 21419.894563519505,\n"," 21921.402444894233,\n"," 20815.782025852608,\n"," 21726.97176509192,\n"," 24726.125776451696,\n"," 22649.24030157046,\n"," 23308.680099877987,\n"," 23452.313273575906,\n"," 23303.550016427307,\n"," 23506.71275316695,\n"," 23404.35368597615,\n"," 23713.80709854263,\n"," 23203.674127928047,\n"," 25033.448567125466,\n"," 22645.911204863103,\n"," 24934.418596642012,\n"," 25124.180900504216,\n"," 23149.00177155771,\n"," 23767.822367628432,\n"," 24318.244905330434,\n"," 22955.553448160856,\n"," 24059.563894204657,\n"," 22137.936143819155,\n"," 23098.21736353873,\n"," 25637.995524624992,\n"," 25341.04423457853,\n"," 23917.07680297976,\n"," 23048.116238975086,\n"," 23651.81946147305,\n"," 24165.197482746946,\n"," 25434.30340693352,\n"," 24366.886684878427,\n"," 25180.902015733467,\n"," 26452.34049062942,\n"," 25033.57081592259,\n"," 24622.975366600942,\n"," 24571.004490345593,\n"," 23969.375131370252,\n"," 24676.50164049825,\n"," 25590.62425936139,\n"," 25236.147413340324,\n"," 25993.82583947135,\n"," 24677.12925709062,\n"," 25129.777057493924,\n"," 24213.184358024646,\n"," 25639.308607686937,\n"," 25235.055906223148,\n"," 26105.026114311197,\n"," 24018.726390544398,\n"," 24169.890963350772,\n"," 24727.326434280574,\n"," 26036.40975451906,\n"," 25688.849789099484,\n"," 25438.955410266884,\n"," 24983.415116003085,\n"," 25181.147604834834,\n"," 24165.08833203524,\n"," 24524.381998964367,\n"," 23760.045379418618,\n"," 26048.170743706527,\n"," 25540.00466891709,\n"," 25643.61896929263,\n"," 26047.513656421997,\n"," 24978.65396195801,\n"," 24668.560926220867,\n"," 24977.60065758995,\n"," 25235.001330867293,\n"," 25237.268391149642,\n"," 25953.206637234525,\n"," 24780.334242297227,\n"," 26354.920493143796,\n"," 24574.6610391881,\n"," 24935.23722697989,\n"," 25235.001330867293,\n"," 25839.523183703124,\n"," 25532.9917356893,\n"," 24264.88454051494,\n"," 25083.67303199334,\n"," 25290.219440796205,\n"," 25590.638448953905,\n"," 25229.32549385803,\n"," 24726.480516264775,\n"," 24980.331608397093,\n"," 24016.024910429416,\n"," 24980.549909820525,\n"," 25378.989244378292,\n"," 24734.039203051147,\n"," 24369.288000536188,\n"," 25693.10666685643,\n"," 25389.085685212085,\n"," 25835.921210216482,\n"," 26710.73443538339,\n"," 24415.883204239497,\n"," 25236.06555030652]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["rewards_train_SP\n"]},{"cell_type":"code","execution_count":null,"id":"qL7V3SFzQ7Gt","metadata":{"id":"qL7V3SFzQ7Gt","outputId":"f2879d43-d8a8-4ae2-8e36-14994bb692f5"},"outputs":[{"data":{"text/plain":["[1310.8527648273762,\n"," 2884.9120360377665,\n"," 4631.77099994802,\n"," 6225.005294748679,\n"," 7799.631982268714,\n"," 9167.824040585812,\n"," 10393.031092879182,\n"," 11545.868228080688,\n"," 12472.85960785788,\n"," 13398.270823581282,\n"," 14531.056318868325,\n"," 15342.874717138538,\n"," 16139.455255412484,\n"," 16870.741057228825,\n"," 17514.021953148673,\n"," 18113.2910331505,\n"," 18642.39729843307,\n"," 19149.538278444026,\n"," 19554.95186339243,\n"," 20102.801533765734,\n"," 20357.11250087547,\n"," 20814.843110452126,\n"," 21245.776889457335,\n"," 21436.09937766737,\n"," 21669.271676663477,\n"," 21934.16899953017,\n"," 22036.307444393242,\n"," 22238.633089374383,\n"," 22228.563394818862,\n"," 22315.52879169085,\n"," 22647.775464984265,\n"," 22917.10234194369,\n"," 23017.099788047297,\n"," 23020.201433140075,\n"," 23083.363235973375,\n"," 23191.546660650733,\n"," 23415.822335279016,\n"," 23510.928770238956,\n"," 23677.92609478841,\n"," 23955.36753437251,\n"," 24063.18786252752,\n"," 24119.166612934863,\n"," 24164.350400675936,\n"," 24144.852873745367,\n"," 24198.017750420655,\n"," 24337.27840131473,\n"," 24427.16530251729,\n"," 24583.831356212697,\n"," 24593.16114630049,\n"," 24646.822737419832,\n"," 24603.458899480313,\n"," 24707.043870300975,\n"," 24759.845073893193,\n"," 24894.363177934993,\n"," 24806.799499195935,\n"," 24743.10864561142,\n"," 24741.530424478336,\n"," 24871.01835748241,\n"," 24952.80150064412,\n"," 25001.416891606397,\n"," 24999.616714046068,\n"," 25017.769803124946,\n"," 24932.501656015975,\n"," 24891.689690310817,\n"," 24778.5252592216,\n"," 24905.489807670092,\n"," 24968.941293794793,\n"," 25036.409061344577,\n"," 25137.519520852322,\n"," 25121.63296496289,\n"," 25076.325761088687,\n"," 25066.453250738814,\n"," 25083.308058751663,\n"," 25098.704091991465,\n"," 25184.154346515774,\n"," 25143.77233609392,\n"," 25264.88715179891,\n"," 25195.86454053783,\n"," 25169.801809182034,\n"," 25176.321761350562,\n"," 25242.64190358582,\n"," 25271.676886796165,\n"," 25170.997652168044,\n"," 25162.265190150574,\n"," 25175.060615215138,\n"," 25216.618398589017,\n"," 25217.889108115916,\n"," 25168.748248930802,\n"," 25149.906584877433,\n"," 25036.51841743263,\n"," 25030.921566671423,\n"," 25065.72833444211,\n"," 25032.559421303013,\n"," 24966.23227922633,\n"," 25038.91971798934,\n"," 25073.936314711616,\n"," 25150.134804262103,\n"," 25306.194767374232,\n"," 25217.16361106076,\n"," 25219.05380498534]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["ma_rewards_train_SP\n"]},{"cell_type":"code","execution_count":null,"id":"eVYd3WD--yDQ","metadata":{"id":"eVYd3WD--yDQ","outputId":"be8f7627-2c05-4370-dc40-0d5561b1a80a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start Testing!\n","Environment：TradingSystem_v0, Algorithm：DQN, Device：cpu,Stock: SP500\n","Stock：SP500，Return：2039.6\n","Finish Testing!\n"]}],"source":["# Testing SP500\n","env, agent = env_agent_config(SP_test, cfg, 'test',\"SP500\")\n","stocks_test_SP, rewards_test_SP = test(cfg, env, agent)\n"]},{"cell_type":"code","execution_count":null,"id":"nsRaVs2DWUYC","metadata":{"id":"nsRaVs2DWUYC","outputId":"b79c62b9-de67-400d-bbf6-c682898bac4d"},"outputs":[{"data":{"text/plain":["'SP500'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["stocks_test_SP"]},{"cell_type":"code","execution_count":null,"id":"31HZXWWYWUe1","metadata":{"id":"31HZXWWYWUe1","outputId":"43aa8155-4a42-4e96-a31f-f4998ec69743"},"outputs":[{"data":{"text/plain":["2039.604905208095"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["rewards_test_SP"]},{"cell_type":"code","execution_count":null,"id":"VqcUlK1t-yIL","metadata":{"id":"VqcUlK1t-yIL","outputId":"6f74f1fc-a0f3-4128-a4ed-d10efc2f99e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start Training!\n","Environment：TradingSystem_v0, Algorithm：DQN, Device：cpu, Stock: MSFT\n","Episode：10/100, Reward：730.2911490806887\n","Episode：20/100, Reward：775.5594298001346\n","Episode：30/100, Reward：771.5692470785041\n","Episode：40/100, Reward：752.2763532588352\n","Episode：50/100, Reward：780.9465252369755\n","Episode：60/100, Reward：783.1291099589578\n","Episode：70/100, Reward：761.8604080121287\n","Episode：80/100, Reward：790.3378611792368\n","Episode：90/100, Reward：782.6664920104855\n","Episode：100/100, Reward：777.337105666515\n","Finish Training!\n"]}],"source":["# Training Microsoft\n","cfg = Config()\n","env, agent = env_agent_config(MSFT_train, cfg, 'train','MSFT')\n","rewards_train_MSFT, ma_rewards_train_MSFT = train(cfg, env, agent)\n"]},{"cell_type":"code","execution_count":null,"id":"iMMvLprcWYIJ","metadata":{"id":"iMMvLprcWYIJ","outputId":"ec0da95e-b0e3-4a1b-de2d-7418ac9acb3a"},"outputs":[{"data":{"text/plain":["[274.63149238009737,\n"," 624.7199883835042,\n"," 616.1498662576872,\n"," 705.1165423727515,\n"," 720.3606679146274,\n"," 713.6365170497909,\n"," 754.7630040835505,\n"," 750.0279629652551,\n"," 749.4825965706339,\n"," 730.2911490806887,\n"," 752.364996350266,\n"," 752.4416095533674,\n"," 744.3643003957287,\n"," 760.3564636618992,\n"," 746.6502411264506,\n"," 784.9601662408476,\n"," 775.6361804876741,\n"," 770.04555652586,\n"," 777.1611084002606,\n"," 775.5594298001346,\n"," 768.6652966132083,\n"," 782.6080783098706,\n"," 802.0496615492868,\n"," 740.9734659786455,\n"," 778.5464724226749,\n"," 774.3261751228639,\n"," 771.3133369826353,\n"," 797.7131555282169,\n"," 783.8388582672565,\n"," 771.5692470785041,\n"," 768.5382266213435,\n"," 779.9802137817269,\n"," 778.0511503635025,\n"," 761.7482894529018,\n"," 751.9912277198815,\n"," 768.9286308687316,\n"," 776.5417581924154,\n"," 781.3588582522319,\n"," 767.0846187609937,\n"," 752.2763532588352,\n"," 808.4676689417031,\n"," 793.3727483862175,\n"," 804.3738916960044,\n"," 794.8741149032337,\n"," 798.4945504142928,\n"," 774.7925051511634,\n"," 791.1692811327689,\n"," 796.9261065946266,\n"," 801.5163078574398,\n"," 780.9465252369755,\n"," 768.7117147966036,\n"," 771.5293937770253,\n"," 790.0678568460302,\n"," 789.8178757665521,\n"," 788.3381630485281,\n"," 768.5566667215967,\n"," 760.1824255488868,\n"," 785.1372977537159,\n"," 807.5372409247849,\n"," 783.1291099589578,\n"," 779.360004296144,\n"," 803.6255446317242,\n"," 765.4752754034467,\n"," 787.1938264315642,\n"," 773.1430850799256,\n"," 776.9294814932615,\n"," 760.4717787343108,\n"," 770.700394904276,\n"," 777.299159961614,\n"," 761.8604080121287,\n"," 774.2326513338828,\n"," 745.449295292121,\n"," 801.5531536868364,\n"," 786.7654421081658,\n"," 799.8130433607142,\n"," 796.1784318494197,\n"," 797.3284053283435,\n"," 775.4192472299912,\n"," 781.7958869096723,\n"," 790.3378611792368,\n"," 760.1444970295407,\n"," 775.3933142278648,\n"," 782.6449756959314,\n"," 793.8413984644088,\n"," 747.879989950551,\n"," 764.4117452649447,\n"," 802.0386112375787,\n"," 783.9010012332534,\n"," 782.8551378450404,\n"," 782.6664920104855,\n"," 787.8590469674906,\n"," 776.0100178602775,\n"," 783.9117937616401,\n"," 764.1404712831242,\n"," 767.5347084400488,\n"," 771.8498871876668,\n"," 777.3857064153644,\n"," 765.8611426643793,\n"," 746.9183873373057,\n"," 777.337105666515]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["rewards_train_MSFT"]},{"cell_type":"code","execution_count":null,"id":"JeDuIgIxWYMk","metadata":{"id":"JeDuIgIxWYMk","outputId":"2cafbec3-c2e0-4fbf-c866-5d931c246c55"},"outputs":[{"data":{"text/plain":["[274.63149238009737,\n"," 309.64034198043805,\n"," 340.29129440816297,\n"," 376.7738192046218,\n"," 411.1325040756224,\n"," 441.38290537303925,\n"," 472.7209152440904,\n"," 500.4516200162069,\n"," 525.3547176716496,\n"," 545.8483608125536,\n"," 566.5000243663249,\n"," 585.0941828850291,\n"," 601.0211946360992,\n"," 616.9547215386791,\n"," 629.9242734974563,\n"," 645.4278627717954,\n"," 658.4486945433832,\n"," 669.6083807416309,\n"," 680.3636535074938,\n"," 689.8832311367579,\n"," 697.7614376844031,\n"," 706.2461017469498,\n"," 715.8264577271835,\n"," 718.3411585523297,\n"," 724.3616899393642,\n"," 729.3581384577142,\n"," 733.5536583102064,\n"," 739.9696080320075,\n"," 744.3565330555324,\n"," 747.0778044578295,\n"," 749.2238466741809,\n"," 752.2994833849355,\n"," 754.8746500827922,\n"," 755.5620140198032,\n"," 755.2049353898111,\n"," 756.5773049377032,\n"," 758.5737502631745,\n"," 760.8522610620803,\n"," 761.4754968319717,\n"," 760.5555824746581,\n"," 765.3467911213626,\n"," 768.1493868478481,\n"," 771.7718373326637,\n"," 774.0820650897208,\n"," 776.523313622178,\n"," 776.3502327750766,\n"," 777.8321376108458,\n"," 779.741534509224,\n"," 781.9190118440456,\n"," 781.8217631833386,\n"," 780.5107583446651,\n"," 779.6126218879011,\n"," 780.6581453837141,\n"," 781.574118421998,\n"," 782.2505228846511,\n"," 780.8811372683457,\n"," 778.8112660963999,\n"," 779.4438692621316,\n"," 782.2532064283969,\n"," 782.340796781453,\n"," 782.0427175329222,\n"," 784.2010002428024,\n"," 782.3284277588668,\n"," 782.8149676261366,\n"," 781.8477793715156,\n"," 781.3559495836902,\n"," 779.2675324987523,\n"," 778.4108187393047,\n"," 778.2996528615356,\n"," 776.6557283765949,\n"," 776.4134206723237,\n"," 773.3170081343035,\n"," 776.1406226895568,\n"," 777.2031046314178,\n"," 779.4640985043474,\n"," 781.1355318388546,\n"," 782.7548191878036,\n"," 782.0212619920225,\n"," 781.9987244837874,\n"," 782.8326381533324,\n"," 780.5638240409531,\n"," 780.0467730596444,\n"," 780.3065933232731,\n"," 781.6600738373867,\n"," 778.2820654487031,\n"," 776.8950334303272,\n"," 779.4093912110524,\n"," 779.8585522132726,\n"," 780.1582107764493,\n"," 780.4090388998529,\n"," 781.1540397066167,\n"," 780.6396375219828,\n"," 780.9668531459486,\n"," 779.2842149596661,\n"," 778.1092643077043,\n"," 777.4833265957005,\n"," 777.473564577667,\n"," 776.3123223863382,\n"," 773.372928881435,\n"," 773.769346559943]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["ma_rewards_train_MSFT"]},{"cell_type":"code","execution_count":null,"id":"YfJZIFxf_ABy","metadata":{"id":"YfJZIFxf_ABy","outputId":"70447b8b-4ede-48bb-f5b9-b0892c138473"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start Testing!\n","Environment：TradingSystem_v0, Algorithm：DQN, Device：cpu,Stock: MSFT\n","Stock：MSFT，Return：57.6\n","Finish Testing!\n"]}],"source":["# Testing Microsoft\n","env, agent = env_agent_config(MSFT_test, cfg, 'test',\"MSFT\")\n","stocks_test_MSFT, rewards_test_MSFT= test(cfg, env, agent)\n"]},{"cell_type":"code","execution_count":null,"id":"KN9UW1BcWaag","metadata":{"id":"KN9UW1BcWaag","outputId":"5f238f25-2004-47f6-c207-733cf86f6f54"},"outputs":[{"data":{"text/plain":["'MSFT'"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["stocks_test_MSFT"]},{"cell_type":"code","execution_count":null,"id":"H_T3bK-qWadm","metadata":{"id":"H_T3bK-qWadm","outputId":"da11cd59-23de-449a-d252-83ddfaf30593"},"outputs":[{"data":{"text/plain":["57.55907878804892"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["rewards_test_MSFT"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}